

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>rlcard.agents &mdash; RLcard 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/doctools.js"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <!-- <link rel="copyright" title="DATA Lab at Texas A&M University"> -->
    <link rel="next" title="rlcard.agents.dmc_agent" href="rlcard.agents.dmc_agent.html" />
    <link rel="prev" title="rlcard.games.uno" href="rlcard.games.uno.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
          <a href="index.html" style="margin: 0px;"><img src="static/imgs/logo_white.png" style="height: initial; width: initial; border-radius: initial; margin: 0px;" alt="home"></a>
          

          
          </a>

          
            
            
          

          
<div role="search" id="test-search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="overview.html#design-principles">Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="overview.html#rlcard-high-level-design">RLCard High-level Design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="overview.html#environments">Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#games">Games</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#agents">Agents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#playing-with-random-agents">Playing with Random Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#deep-q-learning-on-blackjack">Deep-Q Learning on Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#training-cfr-chance-sampling-on-leduc-hold-em">Training CFR (chance sampling) on Leduc Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#having-fun-with-pretrained-leduc-model">Having Fun with Pretrained Leduc Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#training-dmc-on-dou-dizhu">Training DMC on Dou Dizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#evaluating-agents">Evaluating Agents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#evaluating-dmc-on-dou-dizhu">Evaluating DMC on Dou Dizhu</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="games.html">Games in RLCard</a><ul>
<li class="toctree-l2"><a class="reference internal" href="games.html#blackjack">Blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-blackjack">State Representation of Blackjack</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-blackjack">Action Encoding of Blackjack</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-blackjack">Payoff of Blackjack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#leduc-hold-em">Leduc Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-leduc-hold-em">State Representation of Leduc Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-leduc-hold-em">Action Encoding of Leduc Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-leduc-hold-em">Payoff of Leduc Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#limit-texas-hold-em">Limit Texas Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-limit-texas-hold-em">State Representation of Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-limit-texas-hold-em">Action Encoding of Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-limit-texas-hold-em">Payoff of Limit Texas Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#dou-dizhu">Dou Dizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-dou-dizhu">State Representation of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-of-dou-dizhu">State Encoding of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-dou-dizhu">Action Encoding of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff">Payoff</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#mahjong">Mahjong</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-mahjong">State Representation of Mahjong</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-space-of-mahjong">Action Space of Mahjong</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-mahjong">Payoff of Mahjong</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#no-limit-texas-hold-em">No-limit Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="games.html#state-representation-of-no-limit-texas-hold-em">State Representation of No-Limit Texas Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-no-limit-texas-hold-em">Action Encoding of No-Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-no-limit-texas-hold-em">Payoff of No-Limit Texas Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#uno">UNO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-uno">State Representation of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-of-uno">State Encoding of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-uno">Action Encoding of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-uno">Payoff of Uno</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#gin-rummy">Gin Rummy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-gin-rummy">State Representation of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-space-of-gin-rummy">Action Space of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-gin-rummy">Payoff of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#settings">Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#variations">Variations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deep-monte-carlo">Deep Monte-Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deep-q-learning">Deep-Q Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#nfsp">NFSP</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#cfr-chance-sampling">CFR (chance sampling)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-pre-trained-rule-based-models">Adding Pre-trained/Rule-based models</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#developping-algorithms">Developping Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-new-environments">Adding New Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#customizing-environments">Customizing Environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="development.html#state-representation">State Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#action-encoding">Action Encoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#reward-calculation">Reward Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#modifying-game">Modifying Game</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="rlcard.envs.html">rlcard.envs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.blackjack">rlcard.envs.blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.doudizhu">rlcard.envs.doudizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.env">rlcard.envs.env</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.gin_rummy">rlcard.envs.gin_rummy</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.leducholdem">rlcard.envs.leducholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.limitholdem">rlcard.envs.limitholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.mahjong">rlcard.envs.mahjong</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.nolimitholdem">rlcard.envs.nolimitholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.registration">rlcard.envs.registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.uno">rlcard.envs.uno</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.utils.html">rlcard.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.logger">rlcard.utils.logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.seeding">rlcard.utils.seeding</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.utils">rlcard.utils.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.games.html">rlcard.games</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html">rlcard.games.blackjack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.dealer">rlcard.games.blackjack.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.game">rlcard.games.blackjack.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.judger">rlcard.games.blackjack.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.player">rlcard.games.blackjack.player</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html">rlcard.games.doudizhu</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.dealer">rlcard.games.doudizhu.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.game">rlcard.games.doudizhu.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.judger">rlcard.games.doudizhu.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.player">rlcard.games.doudizhu.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.round">rlcard.games.doudizhu.round</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.utils">rlcard.games.doudizhu.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html">rlcard.games.gin_rummy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.dealer">rlcard.games.gin_rummy.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.game">rlcard.games.gin_rummy.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.judge">rlcard.games.gin_rummy.judge</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.player">rlcard.games.gin_rummy.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.round">rlcard.games.gin_rummy.round</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html">rlcard.games.leducholdem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.dealer">rlcard.games.leducholdem.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.game">rlcard.games.leducholdem.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.judger">rlcard.games.leducholdem.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.player">rlcard.games.leducholdem.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.round">rlcard.games.leducholdem.round</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html">rlcard.games.limitholdem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.dealer">rlcard.games.limitholdem.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.game">rlcard.games.limitholdem.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.judger">rlcard.games.limitholdem.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.player">rlcard.games.limitholdem.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.round">rlcard.games.limitholdem.round</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.utils">rlcard.games.limitholdem.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html">rlcard.games.mahjong</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.card">rlcard.games.mahjong.card</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.dealer">rlcard.games.mahjong.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.game">rlcard.games.mahjong.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.judger">rlcard.games.mahjong.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.player">rlcard.games.mahjong.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.round">rlcard.games.mahjong.round</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.utils">rlcard.games.mahjong.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html">rlcard.games.nolimitholdem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.dealer">rlcard.games.nolimitholdem.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.game">rlcard.games.nolimitholdem.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.judger">rlcard.games.nolimitholdem.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.player">rlcard.games.nolimitholdem.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.round">rlcard.games.nolimitholdem.round</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html">rlcard.games.uno</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.card">rlcard.games.uno.card</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.dealer">rlcard.games.uno.dealer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.game">rlcard.games.uno.game</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.judger">rlcard.games.uno.judger</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.player">rlcard.games.uno.player</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.round">rlcard.games.uno.round</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.utils">rlcard.games.uno.utils</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.html#module-rlcard.games.base">rlcard.games.base</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">rlcard.agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.agents.dmc_agent.html">rlcard.agents.dmc_agent</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.file_writer">rlcard.agents.dmc_agent.file_writer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.model">rlcard.agents.dmc_agent.model</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.trainer">rlcard.agents.dmc_agent.trainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.utils">rlcard.agents.dmc_agent.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.agents.human_agents.html">rlcard.agents.human_agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.blackjack_human_agent">rlcard.agents.human_agents.blackjack_human_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.leduc_holdem_human_agent">rlcard.agents.human_agents.leduc_holdem_human_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.limit_holdem_human_agent">rlcard.agents.human_agents.limit_holdem_human_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.nolimit_holdem_human_agent">rlcard.agents.human_agents.nolimit_holdem_human_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.uno_human_agent">rlcard.agents.human_agents.uno_human_agent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.cfr_agent">rlcard.agents.cfr_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.dqn_agent">rlcard.agents.dqn_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.nfsp_agent">rlcard.agents.nfsp_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.random_agent">rlcard.agents.random_agent</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="doctree.html">RLcard</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div style="display: flex; justify-content: space-between">
  <div role="navigation" aria-label="breadcrumbs navigation">

    <ul class="wy-breadcrumbs" style="margin-top: 0.3rem;">
      
        <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          
        <li>RLCard: A Toolkit for Reinforcement Learning in Card Games</li>
      
    </ul>

    
  </div>
  <div style="display: block; width: 7.5rem; max-width: 7.5rem; padding-right: .6rem; float: right;">
    <a href="https://github.com/datamllab/rlcard" title="Go to repository" data-md-source="github" data-md-state="done" style="display: block; padding-right: .6rem; line-height: 1.2">
      <div style="display: inline-block; width: 2.4rem; height: 2.4rem;">
        <i class="fa fa-github" style="font-size: 2rem"></i>  
      </div>
      <div style="display: inline-block; margin-left: -2rem; margin-top: -0.6rem; padding-left: 2rem; vertical-align: middle">
        GitHub
        <ul style="font-size: 0.7rem; list-style-type: none;"><li style="float: left" id="github-stars">... Stars</li></ul>
      </div>
      <script>
        $(document).ready(function(){
        $.ajax({ url: "https://api.github.com/repos/datamllab/rlcard",
                context: document.body,
                success: function(response){
                  console.log(response.stargazers_count);
                  $("#github-stars").html(response.stargazers_count + ' Stars');
                }});
        });
      </script>
    </a>
  </div>
</div>
<hr/>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="rlcard-agents">
<h1>rlcard.agents<a class="headerlink" href="#rlcard-agents" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="rlcard.agents.dmc_agent.html">rlcard.agents.dmc_agent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.file_writer">rlcard.agents.dmc_agent.file_writer</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.model">rlcard.agents.dmc_agent.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.trainer">rlcard.agents.dmc_agent.trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.dmc_agent.html#module-rlcard.agents.dmc_agent.utils">rlcard.agents.dmc_agent.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.agents.human_agents.html">rlcard.agents.human_agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.html">rlcard.agents.human_agents.gin_rummy_human_agent</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.html#subpackages">Subpackages</a><ul>
<li class="toctree-l5"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_cards.html">rlcard.agents.human_agents.gin_rummy_human_agent.gui_cards</a><ul>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_cards.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_cards.card_image">rlcard.agents.human_agents.gin_rummy_human_agent.gui_cards.card_image</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy</a><ul>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.canvas_item">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.canvas_item</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.configurations">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.configurations</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.env_thread">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.env_thread</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_app">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_app</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_debug">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_debug</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_getter">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_getter</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_post_doing_action">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_post_doing_action</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_query">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_query</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_updater">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_canvas_updater</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_frame">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.game_frame</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_discard_pile">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_discard_pile</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_held_pile">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_held_pile</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_player_pane">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_player_pane</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_stock_pile">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_stock_pile</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_to_arrange_held_pile">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.handling_tap_to_arrange_held_pile</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.info_messaging">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.info_messaging</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.menu_bar">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.menu_bar</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.player_type">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.player_type</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.preferences_window">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.preferences_window</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.starting_new_game">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.starting_new_game</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.status_messaging">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.status_messaging</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.utils">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.utils</a></li>
<li class="toctree-l6"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.utils_extra">rlcard.agents.human_agents.gin_rummy_human_agent.gui_gin_rummy.utils_extra</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.agents.human_agents.gin_rummy_human_agent.html#module-rlcard.agents.human_agents.gin_rummy_human_agent.gin_rummy_human_agent">rlcard.agents.human_agents.gin_rummy_human_agent.gin_rummy_human_agent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.blackjack_human_agent">rlcard.agents.human_agents.blackjack_human_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.leduc_holdem_human_agent">rlcard.agents.human_agents.leduc_holdem_human_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.limit_holdem_human_agent">rlcard.agents.human_agents.limit_holdem_human_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.nolimit_holdem_human_agent">rlcard.agents.human_agents.nolimit_holdem_human_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.human_agents.html#module-rlcard.agents.human_agents.uno_human_agent">rlcard.agents.human_agents.uno_human_agent</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="module-rlcard.agents.cfr_agent">
<span id="rlcard-agents-cfr-agent"></span><h2>rlcard.agents.cfr_agent<a class="headerlink" href="#module-rlcard.agents.cfr_agent" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.cfr_agent.</span></span><span class="sig-name descname"><span class="pre">CFRAgent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'./cfr_model'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Implement CFR (chance sampling) algorithm</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.action_probs">
<span class="sig-name descname"><span class="pre">action_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legal_actions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.action_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain the action probabilities of the current state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – state_str</p></li>
<li><p><strong>legal_actions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – List of leagel actions</p></li>
<li><p><strong>player_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The current player</p></li>
<li><p><strong>policy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a>) – The used policy</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>action_probs(numpy.array): The action probabilities
legal_actions (list): Indices of legal actions</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) that contains</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.eval_step">
<span class="sig-name descname"><span class="pre">eval_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a state, predict action based on average policy</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – State representation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted action
info (dict): A dictionary containing information</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.get_state">
<span class="sig-name descname"><span class="pre">get_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">player_id</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.get_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get state_str of the player</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>player_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The player id</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>state (str): The state str
legal_actions (list): Indices of legal actions</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) that contains</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.regret_matching">
<span class="sig-name descname"><span class="pre">regret_matching</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.regret_matching" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply regret matching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obs</strong> (<em>string</em>) – The state_str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Do one iteration of CFR</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.traverse_tree">
<span class="sig-name descname"><span class="pre">traverse_tree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">player_id</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.traverse_tree" title="Permalink to this definition">¶</a></dt>
<dd><p>Traverse the game tree, update the regrets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> – The reach probability of the current node</p></li>
<li><p><strong>player_id</strong> – The player to update the value</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The expected utilities for all the players</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>state_utilities (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.cfr_agent.CFRAgent.update_policy">
<span class="sig-name descname"><span class="pre">update_policy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.cfr_agent.CFRAgent.update_policy" title="Permalink to this definition">¶</a></dt>
<dd><p>Update policy based on the current regrets</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-rlcard.agents.dqn_agent">
<span id="rlcard-agents-dqn-agent"></span><h2>rlcard.agents.dqn_agent<a class="headerlink" href="#module-rlcard.agents.dqn_agent" title="Permalink to this headline">¶</a></h2>
<p>DQN agent</p>
<p>The code is derived from <a class="reference external" href="https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py">https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py</a></p>
<p>Copyright (c) 2019 Matthew Judell
Copyright (c) 2019 DATA Lab at Texas A&amp;M University
Copyright (c) 2016 Denny Britz</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">DQNAgent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">replay_memory_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_memory_init_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_target_estimator_every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon_end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon_decay_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Approximate clone of rlcard.agents.dqn_agent.DQNAgent
that depends on PyTorch instead of Tensorflow</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.eval_step">
<span class="sig-name descname"><span class="pre">eval_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action for evaluation purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an action id
info (dict): A dictionary containing information</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.feed">
<span class="sig-name descname"><span class="pre">feed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ts</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.feed" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Store data in to replay buffer and train the agent. There are two stages.</dt><dd><p>In stage 1, populate the memory without training
In stage 2, train the agent every several timesteps</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ts</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – a list of 5 elements that represent the transition</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.feed_memory">
<span class="sig-name descname"><span class="pre">feed_memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legal_actions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.feed_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed transition to memory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>numpy.array</em>) – the current state</p></li>
<li><p><strong>action</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – the performed action ID</p></li>
<li><p><strong>reward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – the reward received</p></li>
<li><p><strong>next_state</strong> (<em>numpy.array</em>) – the next state after performing the action</p></li>
<li><p><strong>legal_actions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – the legal actions of the next state</p></li>
<li><p><strong>done</strong> (<em>boolean</em>) – whether the episode is finished</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the masked Q-values</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 1-d array where each entry represents a Q value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>q_values (numpy.array)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.set_device">
<span class="sig-name descname"><span class="pre">set_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.set_device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict the action for genrating training data but</dt><dd><p>have the predictions disconnected from the computation graph</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an action id</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.DQNAgent.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the network</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The loss of the current batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>loss (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Estimator">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">Estimator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Approximate clone of rlcard.agents.dqn_agent.Estimator that
uses PyTorch instead of Tensorflow.  All methods input/output np.ndarray.</p>
<p>Q-Value Estimator neural network.
This network is used for both the Q-Network and the Target Network.</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Estimator.predict_nograd">
<span class="sig-name descname"><span class="pre">predict_nograd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator.predict_nograd" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predicts action values, but prediction is not included</dt><dd><p>in the computation graph.  It is used to predict optimal next
actions in the Double-DQN algorithm.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<em>np.ndarray</em>) – (batch, state_len)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>np.ndarray of shape (batch_size, NUM_VALID_ACTIONS) containing the estimated
action values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Estimator.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator.update" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Updates the estimator towards the given targets.</dt><dd><p>In this case y is the target-network estimated
value of the Q-network optimal actions, which
is labeled y in Algorithm 1 of Minh et al. (2015)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> (<em>np.ndarray</em>) – (batch, state_shape) state representation</p></li>
<li><p><strong>a</strong> (<em>np.ndarray</em>) – (batch,) integer sampled actions</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em>) – (batch,) value of optimal actions according to Q-target</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The calculated loss on the batch.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.EstimatorNetwork">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">EstimatorNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.EstimatorNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The function approximation network for Estimator
It is just a series of tanh layers. All in/out are torch.tensor</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.EstimatorNetwork.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.EstimatorNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict action values</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<em>Tensor</em>) – (batch, state_shape)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.EstimatorNetwork.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><span class="pre">bool</span></a></em><a class="headerlink" href="#rlcard.agents.dqn_agent.EstimatorNetwork.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Memory">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">Memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Memory for saving transitions</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Memory.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample a minibatch from the replay memory</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a batch of states
action_batch (list): a batch of actions
reward_batch (list): a batch of rewards
next_state_batch (list): a batch of states
done_batch (list): a batch of dones</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>state_batch (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Memory.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legal_actions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save transition into memory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>numpy.array</em>) – the current state</p></li>
<li><p><strong>action</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – the performed action ID</p></li>
<li><p><strong>reward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – the reward received</p></li>
<li><p><strong>next_state</strong> (<em>numpy.array</em>) – the next state after performing the action</p></li>
<li><p><strong>legal_actions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – the legal actions of the next state</p></li>
<li><p><strong>done</strong> (<em>boolean</em>) – whether the episode is finished</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">Transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legal_actions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.action">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">action</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.action" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.done">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">done</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.done" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.legal_actions">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">legal_actions</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.legal_actions" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.next_state">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">next_state</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.next_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.reward">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">reward</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.reward" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.Transition.state">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="rlcard.agents.dqn_agent.copy_model_parameters">
<span class="sig-prename descclassname"><span class="pre">rlcard.agents.dqn_agent.</span></span><span class="sig-name descname"><span class="pre">copy_model_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sess</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.copy_model_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Copys the model parameters of one estimator to another.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sess</strong> (<em>tf.Session</em>) – Tensorflow Session object</p></li>
<li><p><strong>estimator1</strong> (<a class="reference internal" href="#rlcard.agents.dqn_agent.Estimator" title="rlcard.agents.dqn_agent.Estimator"><em>Estimator</em></a>) – Estimator to copy the paramters from</p></li>
<li><p><strong>estimator2</strong> (<a class="reference internal" href="#rlcard.agents.dqn_agent.Estimator" title="rlcard.agents.dqn_agent.Estimator"><em>Estimator</em></a>) – Estimator to copy the parameters to</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-rlcard.agents.nfsp_agent">
<span id="rlcard-agents-nfsp-agent"></span><h2>rlcard.agents.nfsp_agent<a class="headerlink" href="#module-rlcard.agents.nfsp_agent" title="Permalink to this headline">¶</a></h2>
<p>Neural Fictitious Self-Play (NFSP) agent implemented in TensorFlow.</p>
<p>See the paper <a class="reference external" href="https://arxiv.org/abs/1603.01121">https://arxiv.org/abs/1603.01121</a> for more details.</p>
<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.AveragePolicyNetwork">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.nfsp_agent.</span></span><span class="sig-name descname"><span class="pre">AveragePolicyNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.AveragePolicyNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Approximates the history of action probabilities
given state (average policy). Forward pass returns
log probabilities of actions.</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.AveragePolicyNetwork.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.AveragePolicyNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Log action probabilities of each action from state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<em>Tensor</em>) – (batch, state_shape) state tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(batch, num_actions)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>log_action_probs (Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.AveragePolicyNetwork.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><span class="pre">bool</span></a></em><a class="headerlink" href="#rlcard.agents.nfsp_agent.AveragePolicyNetwork.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.nfsp_agent.</span></span><span class="sig-name descname"><span class="pre">NFSPAgent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_layers_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reservoir_buffer_capacity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anticipatory_param</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rl_learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sl_learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_buffer_size_to_learn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_replay_memory_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_replay_memory_init_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_update_target_estimator_every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_discount_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_epsilon_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_epsilon_end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_epsilon_decay_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_train_every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_mlp_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluate_with</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'average_policy'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>An approximate clone of rlcard.agents.nfsp_agent that uses
pytorch instead of tensorflow.  Note that this implementation
differs from Henrich and Silver (2016) in that the supervised
training minimizes cross-entropy with respect to the stored
action probabilities rather than the realized actions.</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.eval_step">
<span class="sig-name descname"><span class="pre">eval_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Use the average policy for evaluation purpose</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a>) – The current state.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An action id.
info (dict): A dictionary containing information</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.feed">
<span class="sig-name descname"><span class="pre">feed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ts</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.feed" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed data to inner RL agent</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ts</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – A list of 5 elements that represent the transition.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.sample_episode_policy">
<span class="sig-name descname"><span class="pre">sample_episode_policy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.sample_episode_policy" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample average/best_response policy</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.set_device">
<span class="sig-name descname"><span class="pre">set_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.set_device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the action to be taken.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a>) – The current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An action id</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.NFSPAgent.train_sl">
<span class="sig-name descname"><span class="pre">train_sl</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.NFSPAgent.train_sl" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss on sampled transitions and perform a avg-network update.</p>
<p>If there are not enough elements in the buffer, no loss is computed and
<cite>None</cite> is returned instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The average loss obtained on this batch of transitions or <cite>None</cite>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>loss (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.ReservoirBuffer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.nfsp_agent.</span></span><span class="sig-name descname"><span class="pre">ReservoirBuffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reservoir_buffer_capacity</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.ReservoirBuffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Allows uniform sampling over a stream of data.</p>
<p>This class supports the storage of arbitrary elements, such as observation
tensors, integer actions, etc.</p>
<p>See <a class="reference external" href="https://en.wikipedia.org/wiki/Reservoir_sampling">https://en.wikipedia.org/wiki/Reservoir_sampling</a> for more details.</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.ReservoirBuffer.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">element</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.ReservoirBuffer.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Potentially adds <cite>element</cite> to the reservoir buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>element</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><em>object</em></a>) – data to be added to the reservoir buffer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.ReservoirBuffer.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.ReservoirBuffer.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear the buffer</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.ReservoirBuffer.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.ReservoirBuffer.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <cite>num_samples</cite> uniformly sampled from the buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The number of samples to draw.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An iterable over <cite>num_samples</cite> random elements of the buffer.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.9)"><strong>ValueError</strong></a> – If there are less than <cite>num_samples</cite> elements in the buffer</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.Transition">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.nfsp_agent.</span></span><span class="sig-name descname"><span class="pre">Transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">info_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_probs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.nfsp_agent.Transition" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.Transition.action_probs">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">action_probs</span></span><a class="headerlink" href="#rlcard.agents.nfsp_agent.Transition.action_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="rlcard.agents.nfsp_agent.Transition.info_state">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">info_state</span></span><a class="headerlink" href="#rlcard.agents.nfsp_agent.Transition.info_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-rlcard.agents.random_agent">
<span id="rlcard-agents-random-agent"></span><h2>rlcard.agents.random_agent<a class="headerlink" href="#module-rlcard.agents.random_agent" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="rlcard.agents.random_agent.RandomAgent">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">rlcard.agents.random_agent.</span></span><span class="sig-name descname"><span class="pre">RandomAgent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_actions</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>A random agent. Random agents is for running toy examples on the card games</p>
<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.random_agent.RandomAgent.eval_step">
<span class="sig-name descname"><span class="pre">eval_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict the action given the current state for evaluation.</dt><dd><p>Since the random agents are not trained. This function is equivalent to step function</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a>) – An dictionary that represents the current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The action predicted (randomly chosen) by the random agent
probs (list): The list of action probabilities</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="rlcard.agents.random_agent.RandomAgent.step">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action given the curent state in gerenerating training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)"><em>dict</em></a>) – An dictionary that represents the current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The action predicted (randomly chosen) by the random agent</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rlcard.agents.dmc_agent.html" class="btn btn-neutral float-right" title="rlcard.agents.dmc_agent" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rlcard.games.uno.html" class="btn btn-neutral float-left" title="rlcard.games.uno" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright DATA Lab at Texas A&amp;M University

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>