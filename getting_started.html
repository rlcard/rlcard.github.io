

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting Started &mdash; RLcard 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script type="text/javascript" src="static/jquery.js"></script>
        <script type="text/javascript" src="static/underscore.js"></script>
        <script type="text/javascript" src="static/doctools.js"></script>
        <script type="text/javascript" src="static/language_data.js"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <!-- <link rel="copyright" title="DATA Lab at Texas A&M University"> -->
    <link rel="next" title="Games in RLCard" href="games.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> RLcard
          

          
          </a>

          
            
            
          

          
<div role="search" id="test-search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Documentation:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#rlcard-a-toolkit-for-reinforcement-learning-in-card-games">RLCard: A Toolkit for Reinforcement Learning in Card Games</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#available-environments">Available Environments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="overview.html#design-principles">Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="overview.html#rlcard-high-level-design">RLCard High-level Design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="overview.html#environments">Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#games">Games</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#agents">Agents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#playing-with-random-agents">Playing with Random Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deep-q-learning-on-blackjack">Deep-Q Learning on Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deepcfr-on-blackjack">DeepCFR on Blackjack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="games.html">Games in RLCard</a><ul>
<li class="toctree-l2"><a class="reference internal" href="games.html#blackjack">Blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding">State Encoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-decoding">Action Decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff">Payoff</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#limit-texas-hold-em">Limit Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="games.html#no-limit-texas-hold-em">No-limit Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="games.html#dou-dizhu">Dou Dizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state">State</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#id1">State Encoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-abstraction">Action Abstraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#id2">Payoff</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deep-q-learning">Deep-Q Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deepcfr">DeepCFR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="development.html#developping-algorithms">Developping Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-new-environments">Adding New Environments</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rlcard.envs.html">rlcard.envs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.env">rlcard.envs.env</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.registration">rlcard.envs.registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.blackjack">rlcard.envs.blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.doudizhu">rlcard.envs.doudizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.limitholdem">rlcard.envs.limitholdem</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.utils.html">rlcard.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.utils">rlcard.utils.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.logger">rlcard.utils.logger</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.games.html">rlcard.games</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.blackjack.html">rlcard.games.blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.dealer">rlcard.games.blackjack.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.game">rlcard.games.blackjack.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.judger">rlcard.games.blackjack.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.player">rlcard.games.blackjack.player</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.doudizhu.html">rlcard.games.doudizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.dealer">rlcard.games.doudizhu.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.game">rlcard.games.doudizhu.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.judger">rlcard.games.doudizhu.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.player">rlcard.games.doudizhu.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.round">rlcard.games.doudizhu.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.limitholdem.html">rlcard.games.limitholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.dealer">rlcard.games.limitholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.game">rlcard.games.limitholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.judger">rlcard.games.limitholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.player">rlcard.games.limitholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.round">rlcard.games.limitholdem.round</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.agents.html">rlcard.agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.deep_cfr">rlcard.agents.deep_cfr</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.dqn_agent">rlcard.agents.dqn_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.random_agent">rlcard.agents.random_agent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.core.html">rlcard.core</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RLcard</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Getting Started</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="sources/getting_started.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>In this document, we provide some toy examples for getting started. For
more examples, please refer to <a class="reference external" href="examples">examples/</a>.</p>
<div class="section" id="playing-with-random-agents">
<h2>Playing with Random Agents<a class="headerlink" href="#playing-with-random-agents" title="Permalink to this headline">¶</a></h2>
<p>We have set up a random agent that can play randomly on each
environment. An example of applying a random agent on Blackjack is as
follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents.random_agent</span> <span class="kn">import</span> <span class="n">RandomAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils.utils</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Make environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">)</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set up agents</span>
<span class="n">agent_0</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent_0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode_num</span><span class="p">):</span>

    <span class="c1"># Generate data from the environment</span>
    <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Print out the trajectories</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Episode {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">episode</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;State: {}, Action: {}, Reward: {}, Next State: {}, Done: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
</div>
<p>The expected output should look like something as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Episode</span> <span class="mi">0</span>
<span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">19</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">23</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">True</span>

<span class="n">Episode</span> <span class="mi">1</span>
<span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Note that the states and actions are wrapped by <code class="docutils literal notranslate"><span class="pre">env</span></code> in Blackjack. In
this example, the <code class="docutils literal notranslate"><span class="pre">[19,</span> <span class="pre">5]</span></code> suggests the current player obtains score
19 while the card that faces up in the dealer’s hand has score 5. Action
0 means “hit” while action 1 means “stand”. Reward 1 suggests the player
wins while reward -1 suggests the dealer wins. Reward 0 suggests a tie.
The above data can be directly fed into a RL algorithm for training.</p>
</div>
<div class="section" id="deep-q-learning-on-blackjack">
<h2>Deep-Q Learning on Blackjack<a class="headerlink" href="#deep-q-learning-on-blackjack" title="Permalink to this headline">¶</a></h2>
<p>The second example is to use Deep-Q learning to train an agent on
Blackjack. We aim to use this example to show how reinforcement learning
algorithms can be developed and applied in our toolkit. We design a
<code class="docutils literal notranslate"><span class="pre">run</span></code> function which plays one complete game and provides the data for
training RL agents. The example is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents.dqn_agent</span> <span class="kn">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils.utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">rlcard.utils.logger</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="c1"># Make environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">)</span>

<span class="c1"># Set the iterations numbers and how frequently we evaluate/save plot</span>
<span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">save_plot_every</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">1000000</span>

<span class="c1"># Set the the number of steps for collecting normalization statistics</span>
<span class="c1"># and intial memory size</span>
<span class="n">memory_init_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">norm_step</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># Set agents</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
                       <span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">,</span>
                       <span class="n">replay_memory_init_size</span><span class="o">=</span><span class="n">memory_init_size</span><span class="p">,</span>
                       <span class="n">norm_step</span><span class="o">=</span><span class="n">norm_step</span><span class="p">,</span>
                       <span class="n">mlp_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>

    <span class="c1"># Count the number of steps</span>
    <span class="n">step_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Init a Logger to plot the learning curve</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;eposide&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;reward&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s1">&#39;DQN on Blackjack&#39;</span><span class="p">,</span> <span class="n">log_path</span><span class="o">=</span><span class="s1">&#39;./experiments/blackjack_dqn_result/log.txt&#39;</span><span class="p">,</span> <span class="n">csv_path</span><span class="o">=</span><span class="s1">&#39;./experiments/blackjack_dqn_result/performance.csv&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode_num</span><span class="p">):</span>

        <span class="c1"># Generate data from the environment</span>
        <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Feed transitions into agent memory, and train</span>
        <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>
            <span class="n">step_counter</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Train the agent</span>
            <span class="k">if</span> <span class="n">step_counter</span> <span class="o">&gt;</span> <span class="n">memory_init_size</span> <span class="o">+</span> <span class="n">norm_step</span><span class="p">:</span>
                <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># Evaluate the performance</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">eval_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluate_num</span><span class="p">):</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">payoffs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="n">reward</span> <span class="o">+=</span> <span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">########## Evaluation ##########&#39;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;Episode: {} Average reward is {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span><span class="o">/</span><span class="n">evaluate_num</span><span class="p">))</span>

            <span class="c1"># Add point to logger</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">add_point</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span><span class="o">/</span><span class="n">evaluate_num</span><span class="p">)</span>

        <span class="c1"># Make plot</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_plot_every</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">make_plot</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="s1">&#39;./experiments/blackjack_dqn_result/&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.png&#39;</span><span class="p">)</span>

    <span class="c1"># Make the final plot</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">make_plot</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="s1">&#39;./experiments/blackjack_dqn_result/&#39;</span><span class="o">+</span><span class="s1">&#39;final_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The expected output is something like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.377</span>

<span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.401</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">0</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.8321959972381592</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">60</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.7190936803817749</span>
<span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.346</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">195</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.8851202726364136</span>
<span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.211</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">335</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.7604773640632629</span>
<span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.078</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">479</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.57902514934539855</span>
<span class="c1">########## Evaluation ##########</span>
<span class="n">Average</span> <span class="n">reward</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">0.056</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Step</span> <span class="mi">616</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.77693158388137823</span>
</pre></div>
</div>
<p>In Blackjack, the player will get a payoff at the end of the game: 1 if
the player wins, -1 if the player loses, and 0 if it is a tie. The
performance is measured by the average payoff the player obtains by
playing 1000 episodes. The above example shows that the agent achieves
better and better performance during training. The logs and learning
curves are saved in <code class="docutils literal notranslate"><span class="pre">./experiments/blackjack_dqn_result/</span></code>.</p>
</div>
<div class="section" id="deepcfr-on-blackjack">
<h2>DeepCFR on Blackjack<a class="headerlink" href="#deepcfr-on-blackjack" title="Permalink to this headline">¶</a></h2>
<p>The third example is to use Deep Counterfactual Regret Minimization to
train an agent on Blackjack. We aim to use this example to show how CFR
algorithms can be developed and applied in our toolkit. We design
<code class="docutils literal notranslate"><span class="pre">step</span></code> and <code class="docutils literal notranslate"><span class="pre">step_back</span></code> function which allows CFR based algorithms
easily perform the game tree traversal for further optimization. The
example is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents.deep_cfr</span> <span class="kn">import</span> <span class="n">DeepCFR</span>
<span class="kn">from</span> <span class="nn">rlcard.utils.utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># make environment</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">num_iteration</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">train_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">)</span>
<span class="n">test_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">deep_cfr</span> <span class="o">=</span> <span class="n">DeepCFR</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="c1">#</span>
                <span class="n">train_env</span><span class="p">,</span>
                <span class="n">policy_network_layers</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span>
                <span class="n">advantage_network_layers</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span>
                <span class="n">num_traversals</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                <span class="n">num_step</span><span class="o">=</span><span class="mi">40</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                <span class="n">batch_size_advantage</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                <span class="n">batch_size_strategy</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                <span class="n">memory_capacity</span><span class="o">=</span><span class="mf">1e7</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iteration</span><span class="p">):</span>
        <span class="c1"># Train the agent in training environment</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">adv_loss</span><span class="p">,</span> <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">deep_cfr</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># Evaluate the agent</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluate_num</span><span class="p">):</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">player</span> <span class="o">=</span> <span class="n">test_env</span><span class="o">.</span><span class="n">init_game</span><span class="p">()</span>
                <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                    <span class="n">action_prob</span> <span class="o">=</span> <span class="n">deep_cfr</span><span class="o">.</span><span class="n">action_probabilities</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                    <span class="n">action_prob</span> <span class="o">/=</span> <span class="n">action_prob</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_prob</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">action_prob</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">,</span> <span class="n">player</span> <span class="o">=</span> <span class="n">test_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">test_env</span><span class="o">.</span><span class="n">is_over</span><span class="p">():</span>
                        <span class="n">payoffs</span> <span class="o">=</span> <span class="n">test_env</span><span class="o">.</span><span class="n">get_payoffs</span><span class="p">()</span>
                        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">break</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;############## Iteration &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39; #################&#39;</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Reward: &#39;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">/</span><span class="n">evaluate_num</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Advantage Loss: &#39;</span><span class="p">,</span> <span class="n">adv_loss</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Policy Loss: &#39;</span><span class="p">,</span> <span class="n">policy_loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The expected output is shown as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">############## Iteration 0 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">1.0</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">3.872990369796753</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">0.12022944</span>
<span class="c1">############## Iteration 100 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.214</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">32.23717498779297</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">1.9331933</span>
<span class="c1">############## Iteration 200 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.128</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">55.274147033691406</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">6.15625</span>
<span class="c1">############## Iteration 300 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.14</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">58.65533447265625</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">4.0294237</span>
<span class="c1">############## Iteration 400 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.142</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">74.10326385498047</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">14.68907</span>
<span class="c1">############## Iteration 500 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.172</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">165.66090393066406</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">12.746856</span>
<span class="c1">############## Iteration 600 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.187</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">161.6951904296875</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">26.703487</span>
<span class="c1">############## Iteration 700 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.083</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">220.24888610839844</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">11.849184</span>
<span class="c1">############## Iteration 800 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.102</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">251.22244262695312</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">23.548765</span>
<span class="c1">############## Iteration 900 #################</span>
<span class="n">Reward</span><span class="p">:</span>  <span class="o">-</span><span class="mf">0.093</span>
<span class="n">Advantage</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">159.9312286376953</span>
<span class="n">Policy</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">29.732689</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="games.html" class="btn btn-neutral float-right" title="Games in RLCard" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright DATA Lab at Texas A&amp;M University

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>