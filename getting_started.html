

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting Started &mdash; RLcard 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script type="text/javascript" src="static/jquery.js"></script>
        <script type="text/javascript" src="static/underscore.js"></script>
        <script type="text/javascript" src="static/doctools.js"></script>
        <script type="text/javascript" src="static/language_data.js"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <!-- <link rel="copyright" title="DATA Lab at Texas A&M University"> -->
    <link rel="next" title="Games in RLCard" href="games.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
          <a href="index.html" style="margin: 0px;"><img src="static/imgs/logo_white.png" style="height: initial; width: initial; border-radius: initial; margin: 0px;" alt="home"></a>
          

          
          </a>

          
            
            
          

          
<div role="search" id="test-search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Documentation:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="overview.html#design-principles">Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="overview.html#rlcard-high-level-design">RLCard High-level Design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="overview.html#environments">Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#games">Games</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#agents">Agents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#playing-with-random-agents">Playing with Random Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deep-q-learning-on-blackjack">Deep-Q Learning on Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-multiple-processes">Running Multiple Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-cfr-on-leduc-holdem">Training CFR on Leduc Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="#having-fun-with-pretrained-leduc-model">Having Fun with Pretrained Leduc Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#leduc-holdem-as-single-agent-environment">Leduc Hold’em as Single-Agent Environment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="games.html">Games in RLCard</a><ul>
<li class="toctree-l2"><a class="reference internal" href="games.html#blackjack">Blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-blackjack">State Representation of Blackjack</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-blackjack">Action Encoding of Blackjack</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-blackjack">Payoff of Blackjack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#leduc-hold-em">Leduc Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-leduc-hold-em">State Representation of Leduc Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-leduc-hold-em">Action Encoding of Leduc Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-leduc-hold-em">Payoff of Leduc Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#limit-texas-hold-em">Limit Texas Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-limit-texas-hold-em">State Representation of Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-limit-texas-hold-em">Action Encoding of Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-limit-texas-hold-em">Payoff of Limit Texas Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#dou-dizhu">Dou Dizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-dou-dizhu">State Representation of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-of-dou-dizhu">State Encoding of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-abstraction-of-dou-dizhu">Action Abstraction of Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff">Payoff</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#simple-dou-dizhu">Simple Dou Dizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-simple-dou-dizhu">State Representation of Simple Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-of-simple-dou-dizhu">State Encoding of Simple Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-simple-dou-dizhu">Action Encoding of Simple Dou Dizhu</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-simple-dou-dizhu">Payoff of Simple Dou Dizhu</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#mahjong">Mahjong</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-mahjong">State Representation of Mahjong</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-space-of-mahjong">Action Space of Mahjong</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-mahjong">Payoff of Mahjong</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#no-limit-texas-hold-em">No-limit Texas Hold’em</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-no-limit-texas-hold-em">State Representation of No-Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-no-limit-texas-hold-em">Action Encoding of No-Limit Texas Hold’em</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-no-limit-texas-hold-em">Payoff of No-Limit Texas Hold’em</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#uno">UNO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-uno">State Representation of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-of-uno">State Encoding of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding-of-uno">Action Encoding of Uno</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-uno">Payoff of Uno</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#gin-rummy">Gin Rummy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-representation-of-gin-rummy">State Representation of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-space-of-gin-rummy">Action Space of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#payoff-of-gin-rummy">Payoff of Gin Rummy</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#settings">Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#variations">Variations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deep-q-learning">Deep-Q Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#nfsp">NFSP</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#cfr">CFR</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deepcfr">DeepCFR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-pre-trained-rule-based-models">Adding Pre-trained/Rule-based models</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#developping-algorithms">Developping Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-new-environments">Adding New Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#customizing-environments">Customizing Environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="development.html#state-representation">State Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#action-encoding">Action Encoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#reward-calculation">Reward Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="development.html#modifying-game">Modifying Game</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rlcard.envs.html">rlcard.envs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.env">rlcard.envs.env</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.registration">rlcard.envs.registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.blackjack">rlcard.envs.blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.doudizhu">rlcard.envs.doudizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.limitholdem">rlcard.envs.limitholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.gin_rummy">rlcard.envs.gin_rummy</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.leducholdem">rlcard.envs.leducholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.mahjong">rlcard.envs.mahjong</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.nolimitholdem">rlcard.envs.nolimitholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.simpledoudizhu">rlcard.envs.simpledoudizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.uno">rlcard.envs.uno</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.utils.html">rlcard.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.utils">rlcard.utils.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.logger">rlcard.utils.logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.exploitability">rlcard.utils.exploitability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.games.html">rlcard.games</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.blackjack.html">rlcard.games.blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.dealer">rlcard.games.blackjack.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.game">rlcard.games.blackjack.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.judger">rlcard.games.blackjack.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.player">rlcard.games.blackjack.player</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.doudizhu.html">rlcard.games.doudizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.dealer">rlcard.games.doudizhu.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.game">rlcard.games.doudizhu.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.judger">rlcard.games.doudizhu.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.player">rlcard.games.doudizhu.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.round">rlcard.games.doudizhu.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.limitholdem.html">rlcard.games.limitholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.dealer">rlcard.games.limitholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.game">rlcard.games.limitholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.judger">rlcard.games.limitholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.player">rlcard.games.limitholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.round">rlcard.games.limitholdem.round</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.limitholdem.html#module-rlcard.games.limitholdem.utils">rlcard.games.limitholdem.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.nolimitholdem.html">rlcard.games.nolimitholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.dealer">rlcard.games.nolimitholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.game">rlcard.games.nolimitholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.judger">rlcard.games.nolimitholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.player">rlcard.games.nolimitholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.nolimitholdem.html#module-rlcard.games.nolimitholdem.round">rlcard.games.nolimitholdem.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.leducholdem.html">rlcard.games.leducholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.dealer">rlcard.games.leducholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.game">rlcard.games.leducholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.judger">rlcard.games.leducholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.player">rlcard.games.leducholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.leducholdem.html#module-rlcard.games.leducholdem.round">rlcard.games.leducholdem.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.uno.html">rlcard.games.uno</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.dealer">rlcard.games.uno.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.game">rlcard.games.uno.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.judger">rlcard.games.uno.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.player">rlcard.games.uno.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.round">rlcard.games.uno.round</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.card">rlcard.games.uno.card</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.uno.html#module-rlcard.games.uno.utils">rlcard.games.uno.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.mahjong.html">rlcard.games.mahjong</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.dealer">rlcard.games.mahjong.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.game">rlcard.games.mahjong.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.judger">rlcard.games.mahjong.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.player">rlcard.games.mahjong.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.round">rlcard.games.mahjong.round</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.card">rlcard.games.mahjong.card</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.mahjong.html#module-rlcard.games.mahjong.utils">rlcard.games.mahjong.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.gin_rummy.html">rlcard.games.gin_rummy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html">rlcard.games.gin_rummy.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.action_event">rlcard.games.gin_rummy.utils.action_event</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.gin_rummy_error">rlcard.games.gin_rummy.utils.gin_rummy_error</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.melding">rlcard.games.gin_rummy.utils.melding</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.move">rlcard.games.gin_rummy.utils.move</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.scorers">rlcard.games.gin_rummy.utils.scorers</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.settings">rlcard.games.gin_rummy.utils.settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.thinker">rlcard.games.gin_rummy.utils.thinker</a></li>
<li class="toctree-l4"><a class="reference internal" href="rlcard.games.gin_rummy.utils.html#module-rlcard.games.gin_rummy.utils.utils">rlcard.games.gin_rummy.utils.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.dealer">rlcard.games.gin_rummy.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.game">rlcard.games.gin_rummy.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.judge">rlcard.games.gin_rummy.judge</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.player">rlcard.games.gin_rummy.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.gin_rummy.html#module-rlcard.games.gin_rummy.round">rlcard.games.gin_rummy.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.simpledoudizhu.html">rlcard.games.simpledoudizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpledoudizhu.html#module-rlcard.games.simpledoudizhu.dealer">rlcard.games.simpledoudizhu.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpledoudizhu.html#module-rlcard.games.simpledoudizhu.game">rlcard.games.simpledoudizhu.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpledoudizhu.html#module-rlcard.games.simpledoudizhu.player">rlcard.games.simpledoudizhu.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpledoudizhu.html#module-rlcard.games.simpledoudizhu.round">rlcard.games.simpledoudizhu.round</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.agents.html">rlcard.agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.gin_rummy_human_agent">rlcard.agents.gin_rummy_human_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.best_response_agent">rlcard.agents.best_response_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.cfr_agent">rlcard.agents.cfr_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.deep_cfr_agent">rlcard.agents.deep_cfr_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.dqn_agent">rlcard.agents.dqn_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.nfsp_agent">rlcard.agents.nfsp_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.agents.html#module-rlcard.agents.random_agent">rlcard.agents.random_agent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.core.html">rlcard.core</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="doctree.html">RLcard</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div style="display: flex; justify-content: space-between">
  <div role="navigation" aria-label="breadcrumbs navigation">

    <ul class="wy-breadcrumbs" style="margin-top: 0.3rem;">
      
        <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          
        <li>RLCard: A Toolkit for Reinforcement Learning in Card Games</li>
      
    </ul>

    
  </div>
  <div style="display: block; width: 7.5rem; max-width: 7.5rem; padding-right: .6rem; float: right;">
    <a href="https://github.com/datamllab/rlcard" title="Go to repository" data-md-source="github" data-md-state="done" style="display: block; padding-right: .6rem; line-height: 1.2">
      <div style="display: inline-block; width: 2.4rem; height: 2.4rem;">
        <i class="fa fa-github" style="font-size: 2rem"></i>  
      </div>
      <div style="display: inline-block; margin-left: -2rem; margin-top: -0.6rem; padding-left: 2rem; vertical-align: middle">
        GitHub
        <ul style="font-size: 0.7rem; list-style-type: none;"><li style="float: left" id="github-stars">... Stars</li></ul>
      </div>
      <script>
        $(document).ready(function(){
        $.ajax({ url: "https://api.github.com/repos/datamllab/rlcard",
                context: document.body,
                success: function(response){
                  console.log(response.stargazers_count);
                  $("#github-stars").html(response.stargazers_count + ' Stars');
                }});
        });
      </script>
    </a>
  </div>
</div>
<hr/>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>In this document, we provide some toy examples for getting started. All
the examples in this document and even more examples are available in
<a class="reference external" href="https://github.com/datamllab/rlcard/tree/master/examples">examples/</a>.</p>
<div class="section" id="playing-with-random-agents">
<h2>Playing with Random Agents<a class="headerlink" href="#playing-with-random-agents" title="Permalink to this headline">¶</a></h2>
<p>We have set up a random agent that can play randomly on each
environment. An example of applying a random agent on Blackjack is as
follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">RandomAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">set_global_seed</span>

<span class="c1"># Make environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Set up agents</span>
<span class="n">agent_0</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent_0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode_num</span><span class="p">):</span>

    <span class="c1"># Generate data from the environment</span>
    <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Print out the trajectories</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Episode {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">episode</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;State: {}, Action: {}, Reward: {}, Next State: {}, Done: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
</div>
<p>The expected output should look like something as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Episode</span> <span class="mi">0</span>
<span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">True</span>

<span class="n">Episode</span> <span class="mi">1</span>
<span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span>  <span class="mi">5</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Action</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reward</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Next</span> <span class="n">State</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">]),</span> <span class="s1">&#39;legal_actions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">Done</span><span class="p">:</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Note that the states and actions are wrapped by <code class="docutils literal notranslate"><span class="pre">env</span></code> in Blackjack. In
this example, the <code class="docutils literal notranslate"><span class="pre">[20,</span> <span class="pre">3]</span></code> suggests the current player obtains score
20 while the card that faces up in the dealer’s hand has score 3. Action
0 means “hit” while action 1 means “stand”. Reward 1 suggests the player
wins while reward -1 suggests the dealer wins. Reward 0 suggests a tie.
The above data can be directly fed into a RL algorithm for training.</p>
</div>
<div class="section" id="deep-q-learning-on-blackjack">
<h2>Deep-Q Learning on Blackjack<a class="headerlink" href="#deep-q-learning-on-blackjack" title="Permalink to this headline">¶</a></h2>
<p>The second example is to use Deep-Q learning to train an agent on
Blackjack. We aim to use this example to show how reinforcement learning
algorithms can be developed and applied in our toolkit. We design a
<code class="docutils literal notranslate"><span class="pre">run</span></code> function which plays one complete game and provides the data for
training RL agents. The example is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">set_global_seed</span><span class="p">,</span> <span class="n">tournament</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="c1"># Make environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
<span class="n">eval_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="c1"># Set the iterations numbers and how frequently we evaluate/save plot</span>
<span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># The intial memory size</span>
<span class="n">memory_init_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Train the agent every X steps</span>
<span class="n">train_every</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># The paths for saving the logs and learning curves</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./experiments/blackjack_dqn_result/&#39;</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    <span class="c1"># Initialize a global step</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Set up the agents</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
                     <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;dqn&#39;</span><span class="p">,</span>
                     <span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">,</span>
                     <span class="n">replay_memory_init_size</span><span class="o">=</span><span class="n">memory_init_size</span><span class="p">,</span>
                     <span class="n">train_every</span><span class="o">=</span><span class="n">train_every</span><span class="p">,</span>
                     <span class="n">state_shape</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">state_shape</span><span class="p">,</span>
                     <span class="n">mlp_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>
    <span class="n">eval_env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>

    <span class="c1"># Initialize global variables</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="c1"># Init a Logger to plot the learning curve</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode_num</span><span class="p">):</span>

        <span class="c1"># Generate data from the environment</span>
        <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Feed transitions into agent memory, and train the agent</span>
        <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>

        <span class="c1"># Evaluate the performance. Play with random agents.</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_performance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span> <span class="n">tournament</span><span class="p">(</span><span class="n">eval_env</span><span class="p">,</span> <span class="n">evaluate_num</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Close files in the logger</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">close_files</span><span class="p">()</span>

    <span class="c1"># Plot the learning curve</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;DQN&#39;</span><span class="p">)</span>

    <span class="c1"># Save model</span>
    <span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;models/blackjack_dqn&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">save_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>
    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>The expected output is something like below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">1</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.7342</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">100</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">1.0042707920074463</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">136</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.7888197302818298</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">136</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.1406</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">278</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.6946825981140137</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">278</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.1523</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">412</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.62268990278244025</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">412</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.088</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">544</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.69050502777099616</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">544</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.08</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">681</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.61789089441299444</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">681</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.0793</span>
<span class="o">----------------------------------------</span>
</pre></div>
</div>
<p>In Blackjack, the player will get a payoff at the end of the game: 1 if
the player wins, -1 if the player loses, and 0 if it is a tie. The
performance is measured by the average payoff the player obtains by
playing 10000 episodes. The above example shows that the agent achieves
better and better performance during training. The logs and learning
curves are saved in <code class="docutils literal notranslate"><span class="pre">./experiments/blackjack_dqn_result/</span></code>.</p>
</div>
<div class="section" id="running-multiple-processes">
<h2>Running Multiple Processes<a class="headerlink" href="#running-multiple-processes" title="Permalink to this headline">¶</a></h2>
<p>The environments can be run with multiple processes to accelerate the
training. Below is an example to train DQN on Blackjack with multiple
processes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; An example of learning a Deep-Q Agent on Blackjack with multiple processes</span>
<span class="sd">Note that we must use if __name__ == &#39;__main__&#39; for multiprocessing</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">set_global_seed</span><span class="p">,</span> <span class="n">tournament</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Make environment</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;env_num&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">})</span>
    <span class="n">eval_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;blackjack&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;env_num&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">})</span>

    <span class="c1"># Set the iterations numbers and how frequently we evaluate performance</span>
    <span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">iteration_num</span> <span class="o">=</span> <span class="mi">100000</span>

    <span class="c1"># The intial memory size</span>
    <span class="n">memory_init_size</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="c1"># Train the agent every X steps</span>
    <span class="n">train_every</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># The paths for saving the logs and learning curves</span>
    <span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./experiments/blackjack_dqn_result/&#39;</span>

    <span class="c1"># Set a global seed</span>
    <span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

        <span class="c1"># Initialize a global step</span>
        <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># Set up the agents</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
                         <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;dqn&#39;</span><span class="p">,</span>
                         <span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">,</span>
                         <span class="n">replay_memory_init_size</span><span class="o">=</span><span class="n">memory_init_size</span><span class="p">,</span>
                         <span class="n">train_every</span><span class="o">=</span><span class="n">train_every</span><span class="p">,</span>
                         <span class="n">state_shape</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">state_shape</span><span class="p">,</span>
                         <span class="n">mlp_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
        <span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>
        <span class="n">eval_env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>

        <span class="c1"># Initialize global variables</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

        <span class="c1"># Initialize a Logger to plot the learning curve</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iteration_num</span><span class="p">):</span>

            <span class="c1"># Generate data from the environment</span>
            <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># Feed transitions into agent memory, and train the agent</span>
            <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">agent</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>

            <span class="c1"># Evaluate the performance. Play with random agents.</span>
            <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_performance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span> <span class="n">tournament</span><span class="p">(</span><span class="n">eval_env</span><span class="p">,</span> <span class="n">evaluate_num</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Close files in the logger</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">close_files</span><span class="p">()</span>

        <span class="c1"># Plot the learning curve</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;DQN&#39;</span><span class="p">)</span>

        <span class="c1"># Save model</span>
        <span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;models/blackjack_dqn&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">save_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>
        <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>Example output is as follow:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">17</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.7378</span>
<span class="o">----------------------------------------</span>

<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">1100</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.40940183401107797</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">2100</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.44971221685409546</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">2225</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.65466868877410897</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">2225</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.0658</span>
<span class="o">----------------------------------------</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">3100</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.48663979768753053</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">4100</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.71293979883193974</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Copied</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">target</span> <span class="n">network</span><span class="o">.</span>
<span class="n">INFO</span> <span class="o">-</span> <span class="n">Agent</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">step</span> <span class="mi">4440</span><span class="p">,</span> <span class="n">rl</span><span class="o">-</span><span class="n">loss</span><span class="p">:</span> <span class="mf">0.55871248245239263</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">4440</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">0.0736</span>
<span class="o">----------------------------------------</span>
</pre></div>
</div>
</div>
<div class="section" id="training-cfr-on-leduc-holdem">
<h2>Training CFR on Leduc Hold’em<a class="headerlink" href="#training-cfr-on-leduc-holdem" title="Permalink to this headline">¶</a></h2>
<p>To show how we can use <code class="docutils literal notranslate"><span class="pre">step</span></code> and <code class="docutils literal notranslate"><span class="pre">step_back</span></code> to traverse the game
tree, we provide an example of solving Leduc Hold’em with CFR:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">CFRAgent</span>
<span class="kn">from</span> <span class="nn">rlcard</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">set_global_seed</span><span class="p">,</span> <span class="n">tournament</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="c1"># Make environment and enable human mode</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;leduc-holdem&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;allow_step_back&#39;</span><span class="p">:</span><span class="bp">True</span><span class="p">})</span>
<span class="n">eval_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;leduc-holdem&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="c1"># Set the iterations numbers and how frequently we evaluate/save plot</span>
<span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">save_plot_every</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># The paths for saving the logs and learning curves</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./experiments/leduc_holdem_cfr_result/&#39;</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initilize CFR Agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">CFRAgent</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>  <span class="c1"># If we have saved model, we first load the model</span>

<span class="c1"># Evaluate CFR against pre-trained NFSP</span>
<span class="n">eval_env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">agent</span><span class="p">,</span> <span class="n">models</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;leduc-holdem-nfsp&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agents</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Init a Logger to plot the learning curve</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode_num</span><span class="p">):</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">Iteration {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">episode</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="c1"># Evaluate the performance. Play with NFSP agents.</span>
    <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">()</span> <span class="c1"># Save model</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_performance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span> <span class="n">tournament</span><span class="p">(</span><span class="n">eval_env</span><span class="p">,</span> <span class="n">evaluate_num</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Close files in the logger</span>
<span class="n">logger</span><span class="o">.</span><span class="n">close_files</span><span class="p">()</span>

<span class="c1"># Plot the learning curve</span>
<span class="n">logger</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;CFR&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above example, the performance is measured by playing against a
pre-trained NFSP model. The expected output is as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Iteration</span> <span class="mi">0</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">192</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="o">-</span><span class="mf">1.3662</span>
<span class="o">----------------------------------------</span>
<span class="n">Iteration</span> <span class="mi">100</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">19392</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="mf">0.9462</span>
<span class="o">----------------------------------------</span>
<span class="n">Iteration</span> <span class="mi">200</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">38592</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="mf">0.8591</span>
<span class="o">----------------------------------------</span>
<span class="n">Iteration</span> <span class="mi">300</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">57792</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="mf">0.7861</span>
<span class="o">----------------------------------------</span>
<span class="n">Iteration</span> <span class="mi">400</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">76992</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="mf">0.7752</span>
<span class="o">----------------------------------------</span>
<span class="n">Iteration</span> <span class="mi">500</span>
<span class="o">----------------------------------------</span>
  <span class="n">timestep</span>     <span class="o">|</span>  <span class="mi">96192</span>
  <span class="n">reward</span>       <span class="o">|</span>  <span class="mf">0.7215</span>
<span class="o">----------------------------------------</span>
</pre></div>
</div>
<p>We observe that CFR achieves better performance as NFSP. However, CFR
requires traversal of the game tree, which is infeasible in large
environments.</p>
</div>
<div class="section" id="having-fun-with-pretrained-leduc-model">
<h2>Having Fun with Pretrained Leduc Model<a class="headerlink" href="#having-fun-with-pretrained-leduc-model" title="Permalink to this headline">¶</a></h2>
<p>We have designed simple human interfaces to play against the pretrained
model. Leduc Hold’em is a simplified version of Texas Hold’em. Rules can
be found <a class="reference external" href="games.md#leduc-holdem">here</a>. Example of playing against
Leduc Hold’em CFR model is as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">LeducholdemHumanAgent</span> <span class="k">as</span> <span class="n">HumanAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">print_card</span>

<span class="c1"># Make environment</span>
<span class="c1"># Set &#39;record_action&#39; to True because we need it to print results</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;leduc-holdem&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;record_action&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
<span class="n">human_agent</span> <span class="o">=</span> <span class="n">HumanAgent</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">)</span>
<span class="n">cfr_agent</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;leduc-holdem-cfr&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_agents</span><span class="p">([</span><span class="n">human_agent</span><span class="p">,</span> <span class="n">cfr_agent</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt; Leduc Hold&#39;em pre-trained model&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="p">(</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt; Start a new game&quot;</span><span class="p">)</span>

    <span class="n">trajectories</span><span class="p">,</span> <span class="n">payoffs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="c1"># If the human does not take the final action, we need to</span>
    <span class="c1"># print other players action</span>
    <span class="n">final_state</span> <span class="o">=</span> <span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">action_record</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">[</span><span class="s1">&#39;action_record&#39;</span><span class="p">]</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">[</span><span class="s1">&#39;raw_obs&#39;</span><span class="p">]</span>
    <span class="n">_action_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_record</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">action_record</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;current_player&#39;</span><span class="p">]:</span>
            <span class="k">break</span>
        <span class="n">_action_list</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_record</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">_action_list</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt; Player&#39;</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;chooses&#39;</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Let&#39;s take a look at what the agent card is</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;===============     CFR Agent    ===============&#39;</span><span class="p">)</span>
    <span class="n">print_card</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">get_perfect_information</span><span class="p">()[</span><span class="s1">&#39;hand_cards&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;===============     Result     ===============&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;You win {} chips!&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">elif</span> <span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;It is a tie.&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;You lose {} chips!&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">-</span><span class="n">payoffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Press any key to continue...&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Example output is as follow:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt; Leduc Hold&#39;em pre-trained model

&gt;&gt; Start a new game!
&gt;&gt; Agent 1 chooses raise

=============== Community Card ===============
┌─────────┐
│░░░░░░░░░│
│░░░░░░░░░│
│░░░░░░░░░│
│░░░░░░░░░│
│░░░░░░░░░│
│░░░░░░░░░│
│░░░░░░░░░│
└─────────┘
===============   Your Hand    ===============
┌─────────┐
│J        │
│         │
│         │
│    ♥    │
│         │
│         │
│        J│
└─────────┘
===============     Chips      ===============
Yours:   +
Agent 1: +++
=========== Actions You Can Choose ===========
0: call, 1: raise, 2: fold

&gt;&gt; You choose action (integer):
</pre></div>
</div>
<p>We also provide a running demo of a rule-based agent for UNO. Try it by
running <code class="docutils literal notranslate"><span class="pre">examples/uno_human.py</span></code>.</p>
</div>
<div class="section" id="leduc-holdem-as-single-agent-environment">
<h2>Leduc Hold’em as Single-Agent Environment<a class="headerlink" href="#leduc-holdem-as-single-agent-environment" title="Permalink to this headline">¶</a></h2>
<p>We have wrraped the environment as single agent environment by assuming
that other players play with pre-trained models. The interfaces are
exactly the same to OpenAI Gym. Thus, any single-agent algorithm can be
connected to the environment. An example of Leduc Hold’em is as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">rlcard</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.agents</span> <span class="kn">import</span> <span class="n">RandomAgent</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">set_global_seed</span><span class="p">,</span> <span class="n">tournament</span>
<span class="kn">from</span> <span class="nn">rlcard.utils</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="c1"># Make environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;leduc-holdem&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;single_agent_mode&#39;</span><span class="p">:</span><span class="bp">True</span><span class="p">})</span>
<span class="n">eval_env</span> <span class="o">=</span> <span class="n">rlcard</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;leduc-holdem&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;single_agent_mode&#39;</span><span class="p">:</span><span class="bp">True</span><span class="p">})</span>

<span class="c1"># Set the iterations numbers and how frequently we evaluate/save plot</span>
<span class="n">evaluate_every</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">evaluate_num</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># The intial memory size</span>
<span class="n">memory_init_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Train the agent every X steps</span>
<span class="n">train_every</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># The paths for saving the logs and learning curves</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./experiments/leduc_holdem_single_dqn_result/&#39;</span>

<span class="c1"># Set a global seed</span>
<span class="n">set_global_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    <span class="c1"># Initialize a global step</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Set up the agents</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
                     <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;dqn&#39;</span><span class="p">,</span>
                     <span class="n">action_num</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_num</span><span class="p">,</span>
                     <span class="n">replay_memory_init_size</span><span class="o">=</span><span class="n">memory_init_size</span><span class="p">,</span>
                     <span class="n">train_every</span><span class="o">=</span><span class="n">train_every</span><span class="p">,</span>
                     <span class="n">state_shape</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">state_shape</span><span class="p">,</span>
                     <span class="n">mlp_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">])</span>
    <span class="c1"># Initialize global variables</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="c1"># Init a Logger to plot the learning curve</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">timestep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">timestep</span> <span class="o">%</span> <span class="n">evaluate_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">eval_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluate_num</span><span class="p">):</span>
                <span class="n">action</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">eval_step</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_performance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>

    <span class="c1"># Close files in the logger</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">close_files</span><span class="p">()</span>

    <span class="c1"># Plot the learning curve</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;DQN&#39;</span><span class="p">)</span>

    <span class="c1"># Save model</span>
    <span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;models/leduc_holdem_single_dqn&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">save_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>
    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="games.html" class="btn btn-neutral float-right" title="Games in RLCard" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright DATA Lab at Texas A&amp;M University

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>